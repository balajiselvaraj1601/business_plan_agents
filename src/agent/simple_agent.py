"""
Business Plan Analysis Agent

This module implements an AI-powered agent for comprehensive business plan analysis.
It uses LangGraph to orchestrate a multi-step analysis process that examines each
business plan topic in detail, providing strategic insights and actionable recommendations.

The agent combines multiple tools and models:
- Knowledge retrieval for conceptual information
- Web search for current market data
- Structured analysis using predefined prompts
- Executive report generation

The analysis process follows these steps:
1. Load business plan topics from JSON file
2. Analyze each subtopic using strategic analysis prompts
3. Use tools (knowledge search, web search) when needed
4. Generate comprehensive reports
5. Create executive summaries

Configuration:
- Uses Ollama models (granite3.3:8b for general analysis, qwen3:8b for reasoning)
- Integrates with Tavily for web search
- Supports debug mode for testing

Dependencies:
- business_plan.json (generated by planner module)
- meta_data.json (contains API keys)
"""

# Copilot: Do not add any logging for this file.

from langchain_core.messages import ToolMessage
from langgraph.graph import START, StateGraph
from langgraph.prebuilt import ToolNode
from typing_extensions import Literal

from src.prompts.prompt_collection import (
    PROMPT_REPORTING,
)
from src.states.state_collection import BusinessAnalysisState, PlanningResponse, Topic
from src.tools.tool_collection import TOOLS
from src.utils.constants import (
    BUSINESS_PLAN_JSON_PATH,
    DEFAULT_BUSINESS_TYPE,
    DEFAULT_LOCATION,
    FLAG_DEBUG,
    LOG_LEVEL,
    LOG_SEPARATOR_CHAR,
    LOG_SEPARATOR_LENGTH,
    REPORTS_DIR,
)
from src.utils.logging import (
    log_configuration,
    log_data_processing,
    log_debug,
    log_error,
    log_info,
    log_process_end,
    log_process_start,
    log_separator,
    log_success,
    setup_logging,
)
from src.utils.model_collection import llm_general
from src.utils.utils import get_expert_prompt, choose_expert

# ============================================================================
# Module Initialization
# ============================================================================

# Setup logging
setup_logging(LOG_LEVEL)

# Log initialization
log_process_start("Business Plan Analysis Agent")
log_configuration("FLAG_DEBUG", str(FLAG_DEBUG))
log_configuration("LOG_LEVEL", LOG_LEVEL)

# Create analyst_llm by binding tools to the general model
analyst_llm = llm_general.bind_tools(TOOLS)

# ============================================================================
# Data Loading Functions
# ============================================================================


# def save_graph_as_image(app, filename):
#    Image(app.get_graph(xray=True).draw_mermaid_png()).save(filename)


def load_business_plan_data(
    business_plan_json_path=BUSINESS_PLAN_JSON_PATH,
) -> PlanningResponse:
    """
    Load business plan data from JSON file.

    Reads and parses a JSON file containing the structured business plan topics
    generated by the planner module. Validates the data structure and returns
    a properly typed PlanningResponse object.

    Args:
        business_plan_json_path (str): Path to the JSON file containing business
            plan data. Defaults to BUSINESS_PLAN_JSON_PATH constant.

    Returns:
        PlanningResponse: Loaded and validated business plan structure containing
            all topics, subtopics, and metadata.

    Raises:
        FileNotFoundError: If the specified JSON file does not exist.
        ValidationError: If the JSON data doesn't match the expected PlanningResponse schema.

    Example:
        >>> plan = load_business_plan_data("custom_plan.json")
        >>> print(f"Loaded {len(plan.topics)} topics")
    """
    log_data_processing("Loading", "business plan JSON")
    try:
        structured_output = PlanningResponse.load_json(business_plan_json_path)
        log_success("Business plan data loaded successfully")
        return structured_output
    except FileNotFoundError:
        log_error(f"Business plan JSON file not found at {business_plan_json_path}")
        raise FileNotFoundError(
            f"Business plan JSON file not found at {business_plan_json_path}. "
            "Please run the planner first to generate the business plan structure."
        )


def analyst(state: BusinessAnalysisState):
    """
    Perform detailed analysis of business plan topics using AI models.

    This is the core analysis function that processes individual business plan topics.
    It handles tool results from previous steps, manages topic progression, and
    generates detailed analysis for each subtopic using expert prompts.

    The function processes the workflow as follows:
    1. Processes tool results and updates raw information if available
    2. Checks for completion of all topics
    3. Advances to the next topic for analysis
    4. Generates analysis prompts using expert routing
    5. Invokes the analyst LLM for detailed insights
    6. Handles tool calls for additional information gathering

    Args:
        state (BusinessAnalysisState): Current business analysis state containing
            topics, messages, progress tracking, and accumulated information.
            Must have topics_list populated and valid current_topic_index.

    Returns:
        BusinessAnalysisState: Updated state with analysis results, progress
            indicators, and any new messages or tool calls. The state will have
            incremented current_topic_index and potentially new messages from
            the LLM analysis.

    Raises:
        ValueError: If state.topics_list is empty or current_topic_index is invalid.
        RuntimeError: If LLM invocation fails or returns unexpected response format.

    Note:
        This function modifies the input state in-place and returns the same object.
        Tool results are processed before topic advancement to ensure information
        is captured for the current topic before moving to the next one.

    Example:
        >>> initial_state = BusinessAnalysisState(...)
        >>> updated_state = analyst(initial_state)
        >>> print(f"Processed topic: {updated_state.current_topic.topic}")
    """
    log_separator("ANALYSIS NODE", char=LOG_SEPARATOR_CHAR, length=LOG_SEPARATOR_LENGTH)
    log_info("ðŸš€ Analysis Node Processing Started")
    log_debug(f"Business Type: {state.business_type}")
    log_debug(f"Location: {state.location}")
    log_debug(f"Current Topic Index: {state.current_topic_index}")
    log_debug(f"Total Topics: {len(state.topics_list)}")
    log_debug(f"Processed Topics: {len(state.processed_topics)}")
    log_debug(f"Remaining Topics: {len(state.non_processed_topics)}")
    log_debug(f"Raw Information Items: {len(state.raw_information)}")
    log_debug(f"Message Count: {len(state.messages)}")

    # Process tool results if available
    if state.messages and isinstance(state.messages[-1], ToolMessage):
        log_info("ðŸ”§ Processing tool results from previous step")
        tool_msg = state.messages[-1]
        log_debug(f"Tool Message Type: {type(tool_msg)}")
        log_debug(f"Tool Call ID: {getattr(tool_msg, 'tool_call_id', 'N/A')}")

        content = getattr(tool_msg, 'content', '')
        log_debug(f"Tool Content Type: {type(content)}")
        log_debug(f"Tool Content Length: {len(str(content))}")

        if isinstance(content, str):
            state.raw_information.append(content)
            log_info(f"âœ… Added string content to raw information (total: {len(state.raw_information)} items)")
        else:
            content_str = str(content)
            state.raw_information.append(content_str)
            log_info(f"âœ… Converted non-string content to string and added to raw information (total: {len(state.raw_information)} items)")
            log_debug(f"Converted content preview: {content_str[:200]}...")

        # Clear messages after processing
        state.messages = []
        log_debug("ðŸ§¹ Cleared message queue after processing tool results")
        # Don't increment index here - continue with same topic
        log_info("ðŸ”„ Continuing with current topic after tool result processing")

    # Check if work is complete
    if not state.non_processed_topics:
        log_success("ðŸŽ‰ All topics processed successfully - analysis complete")
        log_info("ðŸ“‹ Moving to report generation phase")
        log_debug(f"Final state summary: {len(state.processed_topics)} processed, {len(state.raw_information)} info items")
        return state

    # Process next topic (subtopic)
    previous_index = state.current_topic_index
    state.current_topic_index += 1
    log_info(f"ðŸ“ˆ Advancing to next topic: index {previous_index} â†’ {state.current_topic_index}")

    if state.current_topic:
        current_topic = state.current_topic
        log_info(f"ðŸŽ¯ Processing subtopic: '{current_topic.topic}'")
        log_debug(f"Topic reason: {current_topic.reason or 'No reason provided'}")
        log_debug(f"Topic has {len(current_topic.subtopics)} subtopics")
        log_debug(f"Topic report status: {'Completed' if current_topic.report else 'Pending'}")

        # Expert routing and prompt generation
        log_info("ðŸŽ­ Determining appropriate expert for analysis")
        expert = choose_expert(user_input=current_topic.topic)
        log_info(f"ðŸ‘¨â€ðŸ’¼ Selected expert: {expert.value}")
        log_debug(f"Expert selection reasoning: {expert.name}")

        log_info("ðŸ“ Generating expert-specific analysis prompt")
        prompt = get_expert_prompt(user_input=current_topic.topic)
        log_debug(f"Generated prompt length: {len(prompt)} characters")

        if FLAG_DEBUG:
            log_debug(f"Analysis prompt length: {len(prompt)} characters")
            log_debug(f"Prompt preview: {prompt[:200]}...")
            log_debug(f"Expert prompt template used: {expert.value}")

        log_info("ðŸ¤– Invoking AI model for analysis")
        log_debug("Sending prompt to analyst LLM...")
        output = analyst_llm.invoke(prompt)

        log_success("âœ… AI model response received")
        log_debug(f"Model response type: {type(output)}")
        if hasattr(output, "content"):
            log_debug(f"Response content length: {len(output.content)}")
            log_debug(f"Response preview: {output.content[:200]}...")
        if hasattr(output, "tool_calls") and output.tool_calls:
            log_info(f"ðŸ”§ Model requested {len(output.tool_calls)} tool calls")
            for i, tool_call in enumerate(output.tool_calls):
                log_debug(f"Tool call {i+1}: {tool_call.get('name', 'unknown')}")

        state.messages.append(output)
        log_info(f"ðŸ’¬ Added model response to message queue (total messages: {len(state.messages)})")

        # Log progress
        progress_pct = (len(state.processed_topics) / len(state.topics_list)) * 100
        log_info(f"ðŸ“Š Analysis progress: {progress_pct:.1f}% complete ({len(state.processed_topics)}/{len(state.topics_list)} topics)")
    else:
        log_error("âŒ No current topic available for processing")
        log_debug(f"Current index: {state.current_topic_index}, Topics list length: {len(state.topics_list)}")
        return state

    log_info("âœ… Analysis node processing completed")
    return state


def router_analyst(
    state: BusinessAnalysisState,
) -> Literal["tools", "report_generator", "analyst"]:
    """
    Route the workflow after analyst processing.

    This routing function determines the next step in the workflow based on the
    current state and the results of the analyst processing. It implements the
    conditional logic that controls the flow between analysis, tool execution,
    and report generation phases.

    Routing logic:
    - Routes to "tools" if the analyst's response includes tool calls requiring
      additional information gathering
    - Routes to "report_generator" if all topics have been processed and analysis
      is complete
    - Routes back to "analyst" to continue processing remaining topics

    Args:
        state (BusinessAnalysisState): Current business analysis state containing
            messages from the last analyst invocation and topic processing status.
            Must have valid messages list and topic tracking information.

    Returns:
        Literal["tools", "report_generator", "analyst"]: Routing decision indicating
            the next node in the workflow graph. The return value determines which
            edge the LangGraph will follow in the state machine.

    Note:
        This function is pure and does not modify the input state. It only examines
        the state to make routing decisions based on message content and topic
        completion status.

    Example:
        >>> state = BusinessAnalysisState(messages=[AIMessage(tool_calls=[...])])
        >>> next_node = router_analyst(state)
        >>> print(f"Next node: {next_node}")  # Output: "tools"
    """
    log_separator(
        "ROUTER ANALYST", char=LOG_SEPARATOR_CHAR, length=LOG_SEPARATOR_LENGTH
    )
    log_info("Routing decision in progress")

    # Check if last message has tool calls
    if state.messages and hasattr(state.messages[-1], "tool_calls"):
        log_info("Tool calls detected - routing to tools")
        return "tools"

    # If no more topics, we're done
    if not state.non_processed_topics:
        log_info("No more topics to process - routing to report_generator")
        return "report_generator"

    # Continue processing remaining topics
    log_info("More topics to process - routing back to analyst")
    return "analyst"


def report_generator(state: BusinessAnalysisState):
    """
    Generate a comprehensive executive report from all analysis results.

    This function synthesizes all the individual topic analyses into a cohesive
    executive report suitable for business decision-making. It combines raw
    information from all processed topics, formats it with business context,
    and generates a professional report using AI language models.

    The report generation process follows these steps:
    1. Aggregates all raw analysis information from processed topics
    2. Creates a structured report prompt incorporating business type and location
    3. Invokes the general LLM to generate comprehensive executive summary
    4. Saves the report to a timestamped file in the reports directory
    5. Updates the state with the generated report content

    Args:
        state (BusinessAnalysisState): Business analysis state containing all
            processed information, analysis results, and business context.
            Must have populated raw_information list and valid business_type/location.

    Returns:
        BusinessAnalysisState: Updated state with the generated report content
            stored in the report attribute. The state object is modified in-place
            and returned for workflow continuity.

    Raises:
        ValueError: If state.raw_information is empty or business context is missing.
        RuntimeError: If LLM invocation fails or report generation encounters errors.

    Note:
        The generated report is automatically saved to the file system in a
        structured directory hierarchy based on location and business type.
        The report combines technical analysis with business context for
        executive-level decision making.

    Example:
        >>> state = BusinessAnalysisState(raw_information=[...], business_type="Falooda", location="Sweden")
        >>> updated_state = report_generator(state)
        >>> print(f"Report length: {len(updated_state.report)} characters")
    """
    log_separator(
        "EXECUTIVE REPORT", char=LOG_SEPARATOR_CHAR, length=LOG_SEPARATOR_LENGTH
    )
    log_process_start("Executive Report Generation")
    log_info("ðŸ“‹ Starting comprehensive executive report generation")

    # Input validation and logging
    log_info("ðŸ” Validating input state for report generation")
    if not state.raw_information:
        log_error("âŒ No raw information available for report generation")
        raise ValueError("Cannot generate report: no analysis information available")

    if not state.business_type or not state.location:
        log_error("âŒ Missing business context for report generation")
        log_error(f"Business Type: {state.business_type or 'MISSING'}")
        log_error(f"Location: {state.location or 'MISSING'}")
        raise ValueError("Cannot generate report: business type and location required")

    log_success("âœ… Input validation passed")
    log_info(f"ðŸ“Š Business Context: {state.business_type} in {state.location}")
    log_info(f"ðŸ“ˆ Analysis Data: {len(state.raw_information)} information items")
    log_info(f"ðŸ“‹ Topics Processed: {len(state.processed_topics)}")
    log_info(f"ðŸŽ¯ Total Topics: {len(state.topics_list)}")

    # Content aggregation
    log_info("ðŸ”„ Aggregating analysis content from all processed topics")
    analyses_content = "\n\n".join(state.raw_information)
    log_info(f"ðŸ“ Analysis content compiled: {len(analyses_content)} characters")
    log_debug(f"Content preview: {analyses_content[:300]}...")

    if FLAG_DEBUG:
        log_debug(f"ðŸ“Š Raw information breakdown:")
        for i, info in enumerate(state.raw_information):
            log_debug(f"  Item {i+1}: {len(info)} chars - {info[:100]}...")

    # Prompt construction
    log_info("ðŸ—ï¸ Constructing report generation prompt")
    prompt = PROMPT_REPORTING.format(
        analyses_content=analyses_content,
        business_type=state.business_type,
        location=state.location,
    )
    log_info(f"ðŸ“‹ Report prompt created: {len(prompt)} characters")

    if FLAG_DEBUG:
        log_debug("ðŸ” Prompt structure validation:")
        log_debug(f"  Contains business_type: {'{business_type}' in prompt}")
        log_debug(f"  Contains location: {'{location}' in prompt}")
        log_debug(f"  Contains analyses_content: {'{analyses_content}' in prompt}")
        log_debug(f"  Prompt preview: {prompt[:400]}...")

    # LLM invocation
    log_info("ðŸ¤– Invoking language model for executive report generation")
    log_info(f"ðŸ§  Using model: {llm_general.model_name if hasattr(llm_general, 'model_name') else 'general LLM'}")

    try:
        output = llm_general.invoke(prompt)
        log_success("âœ… LLM invocation completed successfully")
    except Exception as e:
        log_error(f"âŒ LLM invocation failed: {str(e)}")
        raise RuntimeError(f"Report generation failed: {str(e)}")

    # Response processing
    log_info("ðŸ“‹ Processing LLM response")
    if hasattr(output, "content"):
        report_content = output.content
        log_info(f"ðŸ“„ Report content extracted: {len(report_content)} characters")
        log_debug(f"Report preview: {report_content[:300]}...")
    else:
        log_error("âš ï¸ LLM response missing content attribute, converting to string")
        report_content = str(output)
        log_info(f"ðŸ“„ Report content (converted): {len(report_content)} characters")

    # State update
    state.report = report_content
    log_success("âœ… Report content stored in state")
    log_info(f"ðŸ“Š State updated: report attribute set ({len(state.report)} chars)")

    # File saving
    log_info("ðŸ’¾ Preparing to save report to file system")
    topic_str = (
        state.topic.topic if hasattr(state.topic, 'topic') and state.topic
        else (state.topic if isinstance(state.topic, str) else "comprehensive_analysis")
    )
    log_debug(f"ðŸ“ Topic string for filename: '{topic_str}'")

    folder_name = (
        f"{state.location.replace(' ', '_')}_{state.business_type.replace(' ', '_')}"
    )
    log_debug(f"ðŸ“‚ Folder name: '{folder_name}'")

    report_filename = (
        f"{REPORTS_DIR}/{folder_name}/{topic_str.replace(' ', '_')}_report.txt"
    )
    log_info(f"ðŸ“„ Report will be saved to: {report_filename}")

    try:
        state.save_text(report_filename)
        log_success("âœ… Report saved to file system successfully")
        log_info(f"ðŸ’¾ File location: {report_filename}")
    except Exception as e:
        log_error(f"âŒ Failed to save report to file: {str(e)}")
        log_error("âš ï¸ Report generation completed but file save failed")

    # Final summary
    log_success("ðŸŽ‰ Executive report generation completed successfully")
    log_info(f"ðŸ“Š Final Report Summary:")
    log_info(f"  â€¢ Business Type: {state.business_type}")
    log_info(f"  â€¢ Location: {state.location}")
    log_info(f"  â€¢ Topics Analyzed: {len(state.processed_topics)}")
    log_info(f"  â€¢ Report Length: {len(state.report)} characters")
    log_info(f"  â€¢ File Saved: {report_filename}")

    log_process_end("Executive Report Generation")
    return state


def setup_agent_graph(tools, analyst_model=None):
    """
    Create and configure the LangGraph workflow for business analysis.

    This function sets up the complete analysis workflow using LangGraph,
    defining the nodes, edges, and conditional routing logic for the
    multi-step business analysis process. The resulting graph orchestrates
    the flow from topic analysis through tool execution to final report generation.

    The workflow includes these key components:
    - Analyst node: Performs detailed topic analysis using AI models
    - Tools node: Executes search and information gathering tools when needed
    - Report generator node: Creates comprehensive executive summaries
    - Conditional routing: Handles tool calls and workflow progression logic

    Graph structure:
    START â†’ analyst â†’ [tools|report_generator|analyst] â†’ ... â†’ END
    - tools â†’ analyst (loop back for additional information)
    - report_generator â†’ END (terminal node)

    Args:
        tools (List[Tool]): List of tools available for the agent, typically
            including search_knowledge and search_web tools for information gathering.
        analyst_model (Optional[BaseChatModel]): Custom language model for analysis.
            If None, uses the globally configured analyst_llm with bound tools.
            Should be a LangChain chat model compatible with tool binding.

    Returns:
        CompiledGraph: Compiled LangGraph application ready for execution.
            The graph can be invoked with BusinessAnalysisState objects and will
            process topics through the complete analysis workflow.

    Raises:
        ValueError: If tools list is empty or contains invalid tool configurations.
        RuntimeError: If graph compilation fails due to invalid node or edge definitions.

    Note:
        This function modifies the global analyst_llm variable if a custom model
        is provided. The compiled graph includes visualization capabilities that
        can be saved as image files for debugging and documentation.

    Example:
        >>> from src.tools.tool_collection import TOOLS
        >>> graph = setup_agent_graph(TOOLS)
        >>> result = graph.invoke(initial_state)
        >>> print("Analysis complete")
    """
    # Set global analyst_llm for the analyst function
    global analyst_llm
    if analyst_model is not None:
        analyst_llm = analyst_model

    # Other pieces of code (graph setup and execution)
    log_separator("GRAPH SETUP", char=LOG_SEPARATOR_CHAR, length=LOG_SEPARATOR_LENGTH)
    log_info("Creating LangGraph workflow")

    # Simplified graph structure
    graph = StateGraph(BusinessAnalysisState, output_schema=BusinessAnalysisState)

    # Add nodes
    log_info("Adding graph nodes")
    graph.add_node("analyst", analyst)
    graph.add_node("report_generator", report_generator)
    graph.add_node("tools", ToolNode(tools))

    # Define flow
    log_info("Defining graph edges and flow")
    graph.add_edge(START, "analyst")

    graph.add_conditional_edges(
        "analyst",
        router_analyst,
        {
            "tools": "tools",
            "report_generator": "report_generator",
        },
    )

    graph.add_edge("tools", "analyst")

    # Compile and return the graph
    compiled_graph = graph.compile()
    log_success("Graph compiled successfully")
    return compiled_graph


def main():
    """
    Main entry point for the business plan analysis agent.

    This function orchestrates the complete business plan analysis workflow from
    start to finish. It handles the entire pipeline: loading business plan data,
    setting up the analysis graph, executing the multi-step analysis process,
    and saving the final comprehensive report.

    The workflow execution includes:
    1. Tool and model setup with proper tool binding
    2. Creation of the LangGraph analysis workflow
    3. Loading of business plan structure from JSON file
    4. Selection and processing of sample topics for analysis
    5. Graph execution with state management and progress tracking
    6. Generation of executive report with business insights
    7. Automatic saving of results to timestamped report files

    The function processes a subset of topics (first 3 subtopics) from the last
    topic in the business plan for demonstration purposes. In production, this
    would typically process all topics comprehensively.

    Raises:
        FileNotFoundError: If business plan JSON file is missing.
        RuntimeError: If graph execution fails or model invocation encounters errors.
        ValidationError: If loaded data doesn't match expected schema.

    Note:
        This function serves as both a demonstration of the analysis capabilities
        and a template for production usage. The topic selection logic can be
        modified to process different subsets or all available topics.

    Example:
        >>> # Run complete analysis workflow
        >>> main()
        Business Plan Analysis Agent - Process Started
        ...
        Business Plan Analysis Agent - Process Completed Successfully
    """
    # Tool and model setup - analyst_llm is already created above
    log_info("Setting up tools and binding to model")

    # Create the graph
    app = setup_agent_graph(tools=TOOLS)  # Pass tools for graph setup

    # Load business plan data
    structured_output = load_business_plan_data()

    sample_topic = structured_output.topics[-3]
    log_info(f"Selected sample topic: {sample_topic.topic}")

    # Display graph and run final execution
    log_separator(
        "GRAPH EXECUTION", char=LOG_SEPARATOR_CHAR, length=LOG_SEPARATOR_LENGTH
    )
    log_info("Starting graph execution")
    log_info(f"Initial topic: {sample_topic.topic}")
    log_info(f"Subtopics to process: {len(sample_topic.subtopics[:3])}")

    # Convert subtopics to Topic objects
    subtopic_objects = [
        Topic(topic=subtopic, reason="", subtopics=[], report=None)
        for subtopic in sample_topic.subtopics[:3]
    ]

    output = app.invoke(
        BusinessAnalysisState(
            business_type=DEFAULT_BUSINESS_TYPE,
            location=DEFAULT_LOCATION,
            topics_list=subtopic_objects,
            current_topic_index=-1,
            description=sample_topic.reason,
            raw_information=[],
            messages=[],
            report="",
        )
    )

    log_success("Graph execution completed")
    if isinstance(output, dict):
        output = BusinessAnalysisState(**output)

    output.print_structured()

    # Save the final business analysis report
    folder_name = (
        f"{output.location.replace(' ', '_')}_{output.business_type.replace(' ', '_')}"
    )
    report_filename = f"{REPORTS_DIR}/{folder_name}/{output.business_type.replace(' ', '_')}_{output.location.replace(' ', '_')}_business_analysis_report.txt"
    output.save_text(report_filename)

    log_process_end("Business Plan Analysis Agent", success=True)


if __name__ == "__main__":
    main()
